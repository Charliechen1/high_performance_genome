{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import configparser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config/model.conf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = \"../config/main.conf\"\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(config_path)\n",
    "\n",
    "model_conf = configparser.ConfigParser()\n",
    "model_conf.read(conf['path']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset partitions:  ['dev', 'train', 'preprocess', 'test', 'download.sh']\n"
     ]
    }
   ],
   "source": [
    "data_partitions_dirpath = conf['path']['data_part']\n",
    "print('Available dataset partitions: ', os.listdir(data_partitions_dirpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partition \"test\" has 126171 sequences\n",
      "Dataset partition \"dev\" has 126171 sequences\n",
      "Dataset partition \"train\" has 1086741 sequences\n",
      "CPU times: user 8.71 s, sys: 1.2 s, total: 9.9 s\n",
      "Wall time: 9.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_all_shards(partition='dev', data_dir=data_partitions_dirpath):\n",
    "    shards = []\n",
    "    for fn in os.listdir(os.path.join(data_dir, partition)):\n",
    "        with open(os.path.join(data_dir, partition, fn)) as f:\n",
    "            shards.append(pd.read_csv(f, index_col=None))\n",
    "    return pd.concat(shards)\n",
    "\n",
    "test = read_all_shards('test')\n",
    "dev = read_all_shards('dev')\n",
    "train = read_all_shards('train')\n",
    "\n",
    "partitions = {'test': test, 'dev': dev, 'train': train}\n",
    "for name, df in partitions.items():\n",
    "    print('Dataset partition \"%s\" has %d sequences' % (name, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_id</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>family_accession</th>\n",
       "      <th>aligned_sequence</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUF4194</td>\n",
       "      <td>C7MGY1_BRAFD/37-194</td>\n",
       "      <td>PF13835.6</td>\n",
       "      <td>VHLLQGPFLDGRRD...GA.......RYAQLL..RDRTAIEARLAD...</td>\n",
       "      <td>VHLLQGPFLDGRRDGARYAQLLRDRTAIEARLADLFLELIVDDDAQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clathrin_propel</td>\n",
       "      <td>Q7SHV2_NEUCR/257-292</td>\n",
       "      <td>PF01394.20</td>\n",
       "      <td>PPEA.SNDFPVALQVSQKYGIIYL.......ITKYGFIHLYDLE</td>\n",
       "      <td>PPEASNDFPVALQVSQKYGIIYLITKYGFIHLYDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leu_Phe_trans</td>\n",
       "      <td>K7RWT2_ACIA4/30-205</td>\n",
       "      <td>PF03588.14</td>\n",
       "      <td>...VLAALHEGVFPMPIDGDEVPEPLR.GGMGW.....WSPQL......</td>\n",
       "      <td>VLAALHEGVFPMPIDGDEVPEPLRGGMGWWSPQLRARMPLERIRVP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tRNA_anti-codon</td>\n",
       "      <td>EX7L_BACSU/29-104</td>\n",
       "      <td>PF01336.25</td>\n",
       "      <td>IWIK.GELSNVK...............IHT.RGHIYFT.....LKD...</td>\n",
       "      <td>IWIKGELSNVKIHTRGHIYFTLKDENARMQSVMFARQSERLPFKPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSS-motif</td>\n",
       "      <td>PDED_ECOLI/42-242</td>\n",
       "      <td>PF12792.7</td>\n",
       "      <td>NQQRVVQFANHAVE.ELDKVLLPLQA.G...SEVLLP.LIGLPCS....</td>\n",
       "      <td>NQQRVVQFANHAVEELDKVLLPLQAGSEVLLPLIGLPCSVAHLPLR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         family_id         sequence_name family_accession  \\\n",
       "0          DUF4194   C7MGY1_BRAFD/37-194        PF13835.6   \n",
       "1  Clathrin_propel  Q7SHV2_NEUCR/257-292       PF01394.20   \n",
       "2    Leu_Phe_trans   K7RWT2_ACIA4/30-205       PF03588.14   \n",
       "3  tRNA_anti-codon     EX7L_BACSU/29-104       PF01336.25   \n",
       "4        CSS-motif     PDED_ECOLI/42-242        PF12792.7   \n",
       "\n",
       "                                    aligned_sequence  \\\n",
       "0  VHLLQGPFLDGRRD...GA.......RYAQLL..RDRTAIEARLAD...   \n",
       "1       PPEA.SNDFPVALQVSQKYGIIYL.......ITKYGFIHLYDLE   \n",
       "2  ...VLAALHEGVFPMPIDGDEVPEPLR.GGMGW.....WSPQL......   \n",
       "3  IWIK.GELSNVK...............IHT.RGHIYFT.....LKD...   \n",
       "4  NQQRVVQFANHAVE.ELDKVLLPLQA.G...SEVLLP.LIGLPCS....   \n",
       "\n",
       "                                            sequence  \n",
       "0  VHLLQGPFLDGRRDGARYAQLLRDRTAIEARLADLFLELIVDDDAQ...  \n",
       "1               PPEASNDFPVALQVSQKYGIIYLITKYGFIHLYDLE  \n",
       "2  VLAALHEGVFPMPIDGDEVPEPLRGGMGWWSPQLRARMPLERIRVP...  \n",
       "3  IWIKGELSNVKIHTRGHIYFTLKDENARMQSVMFARQSERLPFKPE...  \n",
       "4  NQQRVVQFANHAVEELDKVLLPLQAGSEVLLPLIGLPCSVAHLPLR...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_id</th>\n",
       "      <th>sequence_name</th>\n",
       "      <th>family_accession</th>\n",
       "      <th>aligned_sequence</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EKR</td>\n",
       "      <td>R6QAS0_9FIRM/627-685</td>\n",
       "      <td>PF10371.9</td>\n",
       "      <td>.EEKKLVIPTNRPEMKDFVKNILHPIDHLHGDDLPVSKFV..DRAD...</td>\n",
       "      <td>EEKKLVIPTNRPEMKDFVKNILHPIDHLHGDDLPVSKFVDRADGVY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUF4042</td>\n",
       "      <td>B3MYB3_DROAN/365-542</td>\n",
       "      <td>PF13251.6</td>\n",
       "      <td>KVRISALHLLGSLAKNLPRRFLYGYWHILFPSG.......EHGATN...</td>\n",
       "      <td>KVRISALHLLGSLAKNLPRRFLYGYWHILFPSGEHGATNSHLLLLG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reg_prop</td>\n",
       "      <td>Q8EJN6_SHEON/296-320</td>\n",
       "      <td>PF07494.11</td>\n",
       "      <td>AQANMETLK..AILF...DKSG.LMWVGGSG</td>\n",
       "      <td>AQANMETLKAILFDKSGLMWVGGSG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUF3880</td>\n",
       "      <td>M1WYU2_PSEP2/257-334</td>\n",
       "      <td>PF12996.7</td>\n",
       "      <td>WFVDNPHLILHHYTHPGTDNTAIFTYDAGNL.EPLRRKGFANTY.Y...</td>\n",
       "      <td>WFVDNPHLILHHYTHPGTDNTAIFTYDAGNLEPLRRKGFANTYYLP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UPRTase</td>\n",
       "      <td>B6GYG1_PENRW/502-699</td>\n",
       "      <td>PF14681.6</td>\n",
       "      <td>AT.DRPAAKLLMTPMRDASI.SGSALRKVHGRVGFYLATELCT.QI...</td>\n",
       "      <td>ATDRPAAKLLMTPMRDASISGSALRKVHGRVGFYLATELCTQIMGL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  family_id         sequence_name family_accession  \\\n",
       "0       EKR  R6QAS0_9FIRM/627-685        PF10371.9   \n",
       "1   DUF4042  B3MYB3_DROAN/365-542        PF13251.6   \n",
       "2  Reg_prop  Q8EJN6_SHEON/296-320       PF07494.11   \n",
       "3   DUF3880  M1WYU2_PSEP2/257-334        PF12996.7   \n",
       "4   UPRTase  B6GYG1_PENRW/502-699        PF14681.6   \n",
       "\n",
       "                                    aligned_sequence  \\\n",
       "0  .EEKKLVIPTNRPEMKDFVKNILHPIDHLHGDDLPVSKFV..DRAD...   \n",
       "1  KVRISALHLLGSLAKNLPRRFLYGYWHILFPSG.......EHGATN...   \n",
       "2                    AQANMETLK..AILF...DKSG.LMWVGGSG   \n",
       "3  WFVDNPHLILHHYTHPGTDNTAIFTYDAGNL.EPLRRKGFANTY.Y...   \n",
       "4  AT.DRPAAKLLMTPMRDASI.SGSALRKVHGRVGFYLATELCT.QI...   \n",
       "\n",
       "                                            sequence  \n",
       "0  EEKKLVIPTNRPEMKDFVKNILHPIDHLHGDDLPVSKFVDRADGVY...  \n",
       "1  KVRISALHLLGSLAKNLPRRFLYGYWHILFPSGEHGATNSHLLLLG...  \n",
       "2                          AQANMETLKAILFDKSGLMWVGGSG  \n",
       "3  WFVDNPHLILHHYTHPGTDNTAIFTYDAGNLEPLRRKGFANTYYLP...  \n",
       "4  ATDRPAAKLLMTPMRDASISGSALRKVHGRVGFYLATELCTQIMGL...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '.': 1,\n",
       " 'M': 2,\n",
       " 'H': 3,\n",
       " 'B': 4,\n",
       " 'W': 5,\n",
       " 'R': 6,\n",
       " 'U': 7,\n",
       " 'I': 8,\n",
       " 'O': 9,\n",
       " 'L': 10,\n",
       " 'T': 11,\n",
       " 'D': 12,\n",
       " 'F': 13,\n",
       " 'X': 14,\n",
       " 'Q': 15,\n",
       " 'K': 16,\n",
       " 'N': 17,\n",
       " 'A': 18,\n",
       " 'E': 19,\n",
       " 'Y': 20,\n",
       " 'V': 21,\n",
       " 'Z': 22,\n",
       " 'S': 23,\n",
       " 'P': 24,\n",
       " 'C': 25,\n",
       " 'G': 26}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = None\n",
    "vocab_path = conf['path']['vocab']\n",
    "with open(vocab_path, 'r') as of:\n",
    "    vocab = json.load(of)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(Xs, vocab, max_len):\n",
    "    n_vocab = len(vocab)\n",
    "    for idx, X in enumerate(Xs):\n",
    "        if idx % 10000 == 0:\n",
    "            print(\"Current dealing with data piece no: %s\" % (idx))\n",
    "        tensor = torch.zeros(max_len, n_vocab)\n",
    "        for chidx, ch in enumerate(X[:max_len]):\n",
    "            tensor[chidx][vocab[ch]] = 1\n",
    "        yield to_sparse(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 597 ms, sys: 116 ms, total: 713 ms\n",
      "Wall time: 712 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "SAMPLE_RATE = 400\n",
    "\n",
    "fams = np.array(train[\"family_id\"].value_counts().index)[::SAMPLE_RATE]\n",
    "partition = train[train[\"family_id\"].isin(fams)]\n",
    "max_len = int(model_conf['Preprocess']['MaxLen'])\n",
    "X = partition['aligned_sequence'].values\n",
    "y = partition['family_id'].values\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.25, random_state=41)\n",
    "fam_vocab = {fam: idx for idx, fam in enumerate(fams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#X_train = make_one_hot(X_train_raw, vocab, max_len)\n",
    "#X_test = make_one_hot(X_test_raw, vocab, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dealing with data piece no: 0\n",
      "Current dealing with data piece no: 0\n",
      "CPU times: user 1.44 s, sys: 52 ms, total: 1.49 s\n",
      "Wall time: 745 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train = [y for y in make_one_hot(np.expand_dims(y_train_raw, axis=1), fam_vocab, 1)]\n",
    "y_test = [y for y in make_one_hot(np.expand_dims(y_test_raw, axis=1), fam_vocab, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = {'x_train': '../data/preprocess/x_train',\n",
    "       'x_test': '../data/preprocess/x_test',\n",
    "       'y_train': '../data/preprocess/y_train',\n",
    "       'y_test': '../data/preprocess/y_test'}\n",
    "\n",
    "def store_sparse(x, path):\n",
    "    if os.path.isfile(path):\n",
    "        return\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    torch.save(x.coalesce().values(), path + '/value.pt')\n",
    "    torch.save(x.coalesce().indices(), path + '/indices.pt')\n",
    "    torch.save(x.coalesce().size(), path + '/size.pt')\n",
    "\n",
    "#store_sparse(X_train, path['x_train'])\n",
    "#store_sparse(X_test, path['x_test'])\n",
    "#store_sparse(X_train, path['y_train'])\n",
    "#store_sparse(X_train, path['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aab2eb94510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, vocab, padding):\n",
    "    res = ['<PAD>'] * padding\n",
    "    res[:min(padding, len(seq))] = seq[:min(padding, len(seq))]\n",
    "    # use 0 for padding\n",
    "    idxs = [vocab[w] for w in res]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def one_hot(label, num_class):\n",
    "    ones = torch.sparse.torch.eye(num_class)\n",
    "    return ones.index_select(0, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, batch_size, padding_idx=0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.dropout_layer = nn.Dropout(p=0.2)\n",
    "        self.batch_size = batch_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    # maybe further define our own loss function here, \n",
    "    # currently using MSE (but MSE is not correct)\n",
    "    def loss(self, Y_hat, Y):\n",
    "        Y = Y.view(-1)\n",
    "        Y_hat = Y_hat.view(-1, self.nb_tags)\n",
    "\n",
    "        tag_pad_token = self.tags['<PAD>']\n",
    "        mask = (Y > tag_pad_token).float()\n",
    "\n",
    "        nb_tokens = int(torch.sum(mask).data[0])\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "\n",
    "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
    "\n",
    "        return ce_loss\n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len = X.size()\n",
    "        embeds = self.word_embeddings(X)\n",
    "        lstm_out, _ = self.lstm(embeds.view(batch_size, seq_len, -1))\n",
    "        lstm_out = lstm_out.view(batch_size, seq_len, -1)\n",
    "        # current we just take the last hidden state of the LSTM, later will modify to attention layer\n",
    "        tag_space = self.hidden2tag(lstm_out[:, -1, :])\n",
    "        tag_scores = self.softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "batch no: 0\n",
      "tensor(3.6559e-05, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch no: 100\n",
      "tensor(3.8516e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8417e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0421e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.6011e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7953e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7974e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7374e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7354e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 1\n",
      "batch no: 0\n",
      "tensor(3.6551e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8504e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8406e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0412e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5998e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7943e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7961e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7363e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7341e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 2\n",
      "batch no: 0\n",
      "tensor(3.6542e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8493e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8395e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7922e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7936e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7342e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7315e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 4\n",
      "batch no: 0\n",
      "tensor(3.6525e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8471e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8372e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0384e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5959e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7912e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7923e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7331e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7302e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 5\n",
      "batch no: 0\n",
      "tensor(3.6517e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8460e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8361e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0375e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5946e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7901e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7910e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7320e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7289e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 6\n",
      "batch no: 0\n",
      "tensor(3.6508e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8448e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8350e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0366e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5933e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7891e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7897e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7309e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7276e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 7\n",
      "batch no: 0\n",
      "tensor(3.6500e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8437e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8339e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0357e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5920e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7881e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7885e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7299e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7263e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 8\n",
      "batch no: 0\n",
      "tensor(3.6491e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8426e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8328e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0348e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5908e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7871e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7872e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7288e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7250e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 9\n",
      "batch no: 0\n",
      "tensor(3.6483e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8415e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8317e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0339e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5895e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7860e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7859e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7277e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7237e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 10\n",
      "batch no: 0\n",
      "tensor(3.6474e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8404e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8306e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0329e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5882e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7850e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7847e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7266e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7224e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 11\n",
      "batch no: 0\n",
      "tensor(3.6466e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8393e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8295e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0320e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5869e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7840e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7834e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7256e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7211e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 12\n",
      "batch no: 0\n",
      "tensor(3.6457e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8382e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8284e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0311e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5856e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7829e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7821e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7245e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7198e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 13\n",
      "batch no: 0\n",
      "tensor(3.6449e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8370e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8273e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0302e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5843e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7819e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7809e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7234e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7185e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 14\n",
      "batch no: 0\n",
      "tensor(3.6440e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8359e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8262e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0293e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5830e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7809e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7796e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7223e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7172e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 15\n",
      "batch no: 0\n",
      "tensor(3.6432e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8348e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8251e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0284e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5817e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7799e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7783e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7213e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7159e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 16\n",
      "batch no: 0\n",
      "tensor(3.6423e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8337e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8240e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0275e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5805e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7788e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7771e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7202e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7146e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 17\n",
      "batch no: 0\n",
      "tensor(3.6415e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8326e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8229e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0265e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5792e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7778e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7758e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7191e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7133e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 18\n",
      "batch no: 0\n",
      "tensor(3.6407e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8315e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8218e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0256e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5779e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7768e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7745e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7181e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7120e-05, grad_fn=<MseLossBackward>)\n",
      "epoch: 19\n",
      "batch no: 0\n",
      "tensor(3.6398e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 100\n",
      "tensor(3.8304e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 200\n",
      "tensor(3.8206e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 300\n",
      "tensor(4.0247e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 400\n",
      "tensor(3.5766e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 500\n",
      "tensor(3.7758e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 600\n",
      "tensor(3.7733e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 700\n",
      "tensor(3.7170e-05, grad_fn=<MseLossBackward>)\n",
      "batch no: 800\n",
      "tensor(3.7107e-05, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 20\n",
    "BATCH_SIZE = 5\n",
    "PADDING_SIZE = 300\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(fams), BATCH_SIZE)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "batched_training_data = []\n",
    "\n",
    "# separate data to each batch\n",
    "for idx in range(len(X_train_raw) // BATCH_SIZE + 1):\n",
    "    X_batch = X_train_raw[BATCH_SIZE * idx:BATCH_SIZE * (idx + 1)]\n",
    "    y_data = [y.to_dense() for y in \n",
    "                  y_train[BATCH_SIZE * idx:BATCH_SIZE * (idx + 1)]]\n",
    "    y_batch = torch.Tensor(len(X_batch), *y_data[0].size())\n",
    "    \n",
    "    torch.cat(y_data, out=y_batch)\n",
    "    batched_training_data.append((X_batch, y_batch))\n",
    "\n",
    "for epoch in range(20):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print(\"epoch: %d\" % epoch)\n",
    "    idx = 0\n",
    "    for batch, target in batched_training_data:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_batch = [prepare_sequence(sentence, vocab, PADDING_SIZE)\n",
    "                           for sentence in batch]\n",
    "        sentence_in = torch.stack(sentence_batch)\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        softmax_target = model.softmax(target)\n",
    "        loss = loss_function(tag_scores, softmax_target)\n",
    "        if idx % 100 == 0:\n",
    "            print(\"batch no: %d\" % idx)\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        idx += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0179, 0.0180, 0.0229, 0.0269, 0.0201, 0.0206, 0.0199, 0.0188, 0.0236,\n",
      "         0.0192, 0.0238, 0.0247, 0.0258, 0.0200, 0.0277, 0.0201, 0.0209, 0.0207,\n",
      "         0.0180, 0.0199, 0.0245, 0.0197, 0.0251, 0.0287, 0.0191, 0.0249, 0.0255,\n",
      "         0.0184, 0.0212, 0.0240, 0.0187, 0.0283, 0.0176, 0.0222, 0.0176, 0.0248,\n",
      "         0.0190, 0.0201, 0.0262, 0.0235, 0.0203, 0.0198, 0.0262, 0.0290, 0.0262]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/common/software/pytorch/v1.2.0-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(X_train_raw[idx], vocab, PADDING_SIZE)\n",
    "    input_ = torch.stack([inputs])\n",
    "\n",
    "    tag_scores = model(input_)\n",
    "    _, idx = torch.max(tag_scores, 1)\n",
    "    idx = idx.tolist()[0]\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9b57a44575b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v1.2.0-gpu [conda env:root] *",
   "language": "python",
   "name": "conda-root-pytorch-v1.2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
