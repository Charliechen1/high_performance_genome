{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preprocessing (Families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import configparser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config/model.conf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = \"../config/main.conf\"\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(config_path)\n",
    "\n",
    "model_conf = configparser.ConfigParser()\n",
    "model_conf.read(conf['path']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset partitions:  ['train', 'dev', 'test', 'download.sh']\n"
     ]
    }
   ],
   "source": [
    "data_partitions_dirpath = conf['path']['data_part']\n",
    "print('Available dataset partitions: ', os.listdir(data_partitions_dirpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partition \"test\" has 126171 sequences\n",
      "Dataset partition \"dev\" has 126171 sequences\n",
      "Dataset partition \"train\" has 1086741 sequences\n",
      "CPU times: user 6.48 s, sys: 1.09 s, total: 7.57 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_all_shards(partition='dev', data_dir=data_partitions_dirpath):\n",
    "    shards = []\n",
    "    for fn in os.listdir(os.path.join(data_dir, partition)):\n",
    "        with open(os.path.join(data_dir, partition, fn)) as f:\n",
    "            shards.append(pd.read_csv(f, index_col=None))\n",
    "    return pd.concat(shards)\n",
    "\n",
    "test = read_all_shards('test')\n",
    "dev = read_all_shards('dev')\n",
    "train = read_all_shards('train')\n",
    "\n",
    "partitions = {'test': test, 'dev': dev, 'train': train}\n",
    "for name, df in partitions.items():\n",
    "    print('Dataset partition \"%s\" has %d sequences' % (name, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 200\n",
    "\n",
    "fams = np.array(train[\"family_accession\"].value_counts().index)[::SAMPLE_RATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF13649.6,PF14310.6,PF00478.25,PF00858.24,PF00396.18,PF10143.9,PF13464.6,PF03987.15,PF03456.18,PF11791.8,PF09320.11,PF12802.7,PF07143.11,PF00079.20,PF04050.14,PF04226.13,PF10322.9,PF12705.7,PF03129.20,PF16369.5,PF07870.11,PF04069.12,PF02089.15,PF06491.11,PF11716.8,PF00642.24,PF17242.2,PF13854.6,PF07575.13,PF04346.12,PF14704.6,PF00017.24,PF06923.11,PF18263.1,PF04732.14,PF07986.12,PF06956.11,PF10232.9,PF11454.8,PF00031.21,PF07120.11,PF16540.5,PF02576.17,PF11256.8,PF00954.20,PF09771.9,PF12684.7,PF03237.15,PF12167.8,PF05586.11,PF10673.9,PF18602.1,PF06970.11,PF05918.11,PF04589.13,PF14904.6,PF12286.8,PF13179.6,PF10018.9,PF16764.5,PF15567.6,PF15164.6,PF09639.10,PF11353.8,PF13846.6,PF16243.5,PF15774.5,PF10843.8,PF08067.11,PF13842.6,PF13016.6,PF08087.11,PF15081.6,PF09715.10,PF17298.2,PF11989.8,PF03043.14,PF15814.5,PF03668.15,PF05418.11,PF07028.11,PF06543.12,PF09499.10,PF11246.8,PF06453.11,PF17470.2,PF04311.13,PF06301.11,PF12212.8,PF06919.11\n"
     ]
    }
   ],
   "source": [
    "fam_list = ','.join(fams)\n",
    "print(fam_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contact Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Bio.PDB\n",
    "import os \n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/contact_maps.conf\"\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read(config_path)\n",
    "\n",
    "filename = conf['path']['pdb_codes']\n",
    "pdb_dir = conf['path']['pdb_dir']\n",
    "contact_map_dir = conf['path']['contact_maps_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_residue_dist(residue_one, residue_two) :\n",
    "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
    "    if Bio.PDB.is_aa(residue_one) and Bio.PDB.is_aa(residue_two):\n",
    "        diff_vector  = residue_one[\"CA\"].coord - residue_two[\"CA\"].coord\n",
    "        return np.sqrt(np.sum(diff_vector * diff_vector))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calc_dist_matrix(chain) :\n",
    "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
    "    answer = np.zeros((len(chain), len(chain)), np.float)\n",
    "    for row, residue_one in enumerate(chain) :\n",
    "        for col, residue_two in enumerate(chain) :\n",
    "            answer[row, col] = calc_residue_dist(residue_one, residue_two)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDB structure '1A28'...\n",
      "CPU times: user 3.31 s, sys: 91.3 ms, total: 3.4 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pdb_file = open(filename, 'r')\n",
    "pdb_list = pdb_file.read()\n",
    "for pdb_code in pdb_list.split(', ')[0:1]: #change to all codes when deployed\n",
    "    try:\n",
    "        pdbl = Bio.PDB.PDBList()\n",
    "        pdb_path = pdbl.retrieve_pdb_file(pdb_code, pdir = pdb_dir, file_format = 'pdb', overwrite = True)\n",
    "        structure = Bio.PDB.PDBParser(QUIET = True).get_structure(pdb_code, pdb_path)\n",
    "        model = structure[0]\n",
    "        sequence = Bio.PDB.Selection.unfold_entities(model, 'R')\n",
    "        dist_matrix = calc_dist_matrix(sequence)\n",
    "        contact_map = np.array((dist_matrix < 8.0) & (dist_matrix > 0.01))*1\n",
    "        sparse_contact_map = scipy.sparse.coo_matrix(contact_map)\n",
    "        scipy.sparse.save_npz(contact_map_dir + '/' + pdb_code + '.npz', sparse_contact_map)\n",
    "    except:\n",
    "        print('Failed for PDB code {}'.format(pdb_code)) #this should probably use a logger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-biopy]",
   "language": "python",
   "name": "conda-env-.conda-biopy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
